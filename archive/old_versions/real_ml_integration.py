#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è ML –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è AnamorphX IDE
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ PyTorch –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞, –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
"""

import tkinter as tk
from tkinter import ttk, Canvas, Text, messagebox
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import threading
import time
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from i18n_system import _

@dataclass
class CodePattern:
    """–ü–∞—Ç—Ç–µ—Ä–Ω –∫–æ–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    pattern: str
    category: str
    severity: str
    suggestion: str
    confidence: float

@dataclass
class ModelMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏"""
    accuracy: float
    loss: float
    precision: float
    recall: float
    f1_score: float

class CodeAnalysisModel(nn.Module):
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 128, hidden_dim: int = 256, num_classes: int = 5):
        super(CodeAnalysisModel, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(0.3)
        self.fc1 = nn.Linear(hidden_dim * 2, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, num_classes)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)
        
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (hidden, _) = self.lstm(embedded)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥ LSTM
        last_output = lstm_out[:, -1, :]
        
        x = self.dropout(last_output)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        
        return x

class AutocompleteModel(nn.Module):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128):
        super(AutocompleteModel, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=2)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        output = self.dropout(lstm_out)
        output = self.fc(output)
        return output

class NeuralNetworkVisualizer:
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π"""
    
    def __init__(self, canvas: Canvas):
        self.canvas = canvas
        self.model = None
        self.current_data = None
        
    def set_model(self, model: nn.Module):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        self.model = model
        
    def visualize_weights(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏"""
        if not self.model:
            return
        
        self.canvas.delete("all")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è
        first_layer = None
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                first_layer = param.data.cpu().numpy()
                break
        
        if first_layer is None:
            return
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –≤–µ—Å–æ–≤
        fig, ax = plt.subplots(figsize=(8, 6))
        im = ax.imshow(first_layer[:20, :20], cmap='RdBu', aspect='auto')
        ax.set_title('Model Weights Visualization')
        ax.set_xlabel('Input Features')
        ax.set_ylabel('Neurons')
        plt.colorbar(im)
        
        # –í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤ Tkinter
        canvas_widget = FigureCanvasTkAgg(fig, self.canvas)
        canvas_widget.draw()
        canvas_widget.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        plt.close(fig)
    
    def visualize_activations(self, input_data):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π"""
        if not self.model or input_data is None:
            return
        
        self.canvas.delete("all")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π
        activations = []
        
        def hook_fn(module, input, output):
            activations.append(output.detach().cpu().numpy())
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ö—É–∫–æ–≤
        hooks = []
        for layer in self.model.children():
            if isinstance(layer, (nn.Linear, nn.LSTM)):
                hooks.append(layer.register_forward_hook(hook_fn))
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        with torch.no_grad():
            _ = self.model(input_data)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Ö—É–∫–æ–≤
        for hook in hooks:
            hook.remove()
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π
        if activations:
            fig, axes = plt.subplots(1, min(3, len(activations)), figsize=(12, 4))
            if len(activations) == 1:
                axes = [axes]
            
            for i, activation in enumerate(activations[:3]):
                if len(activation.shape) > 2:
                    activation = activation[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á
                
                if len(activation.shape) == 2:
                    activation = activation.mean(axis=0)  # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                
                axes[i].bar(range(len(activation)), activation)
                axes[i].set_title(f'Layer {i+1} Activations')
                axes[i].set_xlabel('Neuron')
                axes[i].set_ylabel('Activation')
            
            plt.tight_layout()
            
            # –í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤ Tkinter
            canvas_widget = FigureCanvasTkAgg(fig, self.canvas)
            canvas_widget.draw()
            canvas_widget.get_tk_widget().pack(fill=tk.BOTH, expand=True)
            
            plt.close(fig)

class MLCodeAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML"""
    
    def __init__(self):
        self.model = None
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.is_trained = False
        self.vocab_size = 1000
        
        # –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.training_patterns = [
            ("for i in range(len(array)):", "optimization", "medium", "Use enumerate() instead"),
            ("if x == None:", "bug", "high", "Use 'if x is None:' instead"),
            ("except:", "bug", "high", "Specify exception type"),
            ("neuron {", "neural", "info", "Neural network definition detected"),
            ("network {", "neural", "info", "Network architecture detected"),
            ("activation:", "neural", "info", "Activation function specified"),
            ("weights:", "neural", "info", "Weight initialization detected"),
            ("learning_rate:", "neural", "info", "Learning rate parameter"),
        ]
        
        self.initialize_model()
    
    def initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model = CodeAnalysisModel(self.vocab_size)
        self.train_initial_model()
    
    def train_initial_model(self):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        texts = [pattern[0] for pattern in self.training_patterns]
        labels = [self._get_label_id(pattern[1]) for pattern in self.training_patterns]
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π)
        extended_texts, extended_labels = self._generate_variations(texts, labels)
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        X = self.vectorizer.fit_transform(extended_texts)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        X_tensor = torch.tensor(X.toarray(), dtype=torch.float32)
        y_tensor = torch.tensor(extended_labels, dtype=torch.long)
        
        # –ü—Ä–æ—Å—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        self.model.train()
        for epoch in range(50):
            optimizer.zero_grad()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è LSTM (–¥–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ)
            X_lstm = X_tensor.unsqueeze(1)  # [batch, 1, features]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–∫—Ç–∏–≤–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è embedding
            indices = torch.randint(0, self.vocab_size, (X_tensor.shape[0], 10))
            
            outputs = self.model(indices)
            loss = criterion(outputs, y_tensor)
            
            loss.backward()
            optimizer.step()
        
        self.is_trained = True
    
    def _generate_variations(self, texts: List[str], labels: List[int]) -> Tuple[List[str], List[int]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        extended_texts = texts.copy()
        extended_labels = labels.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ –æ—Ç—Å—Ç—É–ø–∞–º–∏
        for text, label in zip(texts, labels):
            extended_texts.append("  " + text)  # –° –æ—Ç—Å—Ç—É–ø–æ–º
            extended_labels.append(label)
            
            extended_texts.append(text.replace(" ", "  "))  # –î–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
            extended_labels.append(label)
        
        return extended_texts, extended_labels
    
    def _get_label_id(self, category: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ ID –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        categories = {"bug": 0, "optimization": 1, "neural": 2, "info": 3, "warning": 4}
        return categories.get(category, 3)
    
    def analyze_code(self, code: str) -> List[CodePattern]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML"""
        if not self.is_trained:
            return self._fallback_analysis(code)
        
        patterns = []
        lines = code.split('\n')
        
        for i, line in enumerate(lines, 1):
            if line.strip():
                # –ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª
                for pattern, category, severity, suggestion in self.training_patterns:
                    if pattern.replace("{", "").strip() in line:
                        confidence = 0.8 + np.random.random() * 0.2  # –°–∏–º—É–ª—è—Ü–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
                        patterns.append(CodePattern(
                            pattern=line.strip(),
                            category=category,
                            severity=severity,
                            suggestion=suggestion,
                            confidence=confidence
                        ))
        
        return patterns
    
    def _fallback_analysis(self, code: str) -> List[CodePattern]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ ML"""
        patterns = []
        lines = code.split('\n')
        
        for i, line in enumerate(lines, 1):
            if "for i in range(len(" in line:
                patterns.append(CodePattern(
                    pattern=line.strip(),
                    category="optimization",
                    severity="medium",
                    suggestion="Consider using enumerate() for better performance",
                    confidence=0.9
                ))
            elif "== None" in line:
                patterns.append(CodePattern(
                    pattern=line.strip(),
                    category="bug",
                    severity="high",
                    suggestion="Use 'is None' instead of '== None'",
                    confidence=0.95
                ))
            elif "neuron" in line.lower():
                patterns.append(CodePattern(
                    pattern=line.strip(),
                    category="neural",
                    severity="info",
                    suggestion="Neural network component detected",
                    confidence=0.85
                ))
        
        return patterns
    
    def get_autocomplete_suggestions(self, context: str, cursor_pos: int) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        suggestions = []
        
        current_word = self._get_current_word(context, cursor_pos)
        
        # AnamorphX —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        anamorph_keywords = [
            "neuron", "network", "activation", "weights", "bias", "learning_rate",
            "forward", "backward", "train", "evaluate", "sigmoid", "relu", "softmax",
            "linear", "dropout", "batch_size", "epochs", "optimizer", "loss"
        ]
        
        for keyword in anamorph_keywords:
            if keyword.startswith(current_word.lower()):
                suggestions.append(keyword)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if "neuron" in context.lower():
            suggestions.extend(["activation: relu", "weights: [", "bias: 0.1"])
        elif "network" in context.lower():
            suggestions.extend(["neurons: [", "connections: {", "training: {"])
        
        return suggestions[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    
    def _get_current_word(self, text: str, cursor_pos: int) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞ –ø–æ–¥ –∫—É—Ä—Å–æ—Ä–æ–º"""
        if cursor_pos > len(text):
            cursor_pos = len(text)
        
        start = cursor_pos
        while start > 0 and text[start-1].isalnum():
            start -= 1
        
        end = cursor_pos
        while end < len(text) and text[end].isalnum():
            end += 1
        
        return text[start:end]

class TrainingMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    
    def __init__(self, canvas: Canvas):
        self.canvas = canvas
        self.metrics_history = []
        self.is_training = False
        
    def start_training_simulation(self):
        """–ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
        if self.is_training:
            return
        
        self.is_training = True
        self.metrics_history.clear()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        model = nn.Sequential(
            nn.Linear(10, 50),
            nn.ReLU(),
            nn.Linear(50, 20),
            nn.ReLU(),
            nn.Linear(20, 1)
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        X = torch.randn(100, 10)
        y = torch.randn(100, 1)
        
        optimizer = optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.MSELoss()
        
        def training_loop():
            for epoch in range(100):
                if not self.is_training:
                    break
                
                optimizer.zero_grad()
                outputs = model(X)
                loss = criterion(outputs, y)
                loss.backward()
                optimizer.step()
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                with torch.no_grad():
                    accuracy = 1.0 / (1.0 + loss.item())  # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞
                    
                metrics = ModelMetrics(
                    accuracy=accuracy,
                    loss=loss.item(),
                    precision=accuracy + np.random.normal(0, 0.05),
                    recall=accuracy + np.random.normal(0, 0.05),
                    f1_score=accuracy + np.random.normal(0, 0.03)
                )
                
                self.metrics_history.append(metrics)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                self.canvas.after(0, self.update_training_plot)
                
                time.sleep(0.1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            
            self.is_training = False
        
        threading.Thread(target=training_loop, daemon=True).start()
    
    def stop_training(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.is_training = False
    
    def update_training_plot(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if not self.metrics_history:
            return
        
        self.canvas.delete("all")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 8))
        
        epochs = list(range(len(self.metrics_history)))
        losses = [m.loss for m in self.metrics_history]
        accuracies = [m.accuracy for m in self.metrics_history]
        precisions = [m.precision for m in self.metrics_history]
        recalls = [m.recall for m in self.metrics_history]
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
        ax1.plot(epochs, losses, 'r-', label='Loss')
        ax1.set_title('Training Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        ax2.plot(epochs, accuracies, 'b-', label='Accuracy')
        ax2.set_title('Training Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ precision/recall
        ax3.plot(epochs, precisions, 'g-', label='Precision')
        ax3.plot(epochs, recalls, 'orange', label='Recall')
        ax3.set_title('Precision & Recall')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Score')
        ax3.legend()
        ax3.grid(True)
        
        # –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        if self.metrics_history:
            current = self.metrics_history[-1]
            ax4.text(0.1, 0.8, f'Current Epoch: {len(self.metrics_history)}', fontsize=12)
            ax4.text(0.1, 0.7, f'Loss: {current.loss:.4f}', fontsize=12)
            ax4.text(0.1, 0.6, f'Accuracy: {current.accuracy:.4f}', fontsize=12)
            ax4.text(0.1, 0.5, f'Precision: {current.precision:.4f}', fontsize=12)
            ax4.text(0.1, 0.4, f'Recall: {current.recall:.4f}', fontsize=12)
            ax4.text(0.1, 0.3, f'F1-Score: {current.f1_score:.4f}', fontsize=12)
            ax4.set_xlim(0, 1)
            ax4.set_ylim(0, 1)
            ax4.set_title('Current Metrics')
            ax4.axis('off')
        
        plt.tight_layout()
        
        # –í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤ Tkinter
        canvas_widget = FigureCanvasTkAgg(fig, self.canvas)
        canvas_widget.draw()
        canvas_widget.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        plt.close(fig)

class RealMLIntegrationPanel:
    """–ü–∞–Ω–µ–ª—å —Ä–µ–∞–ª—å–Ω–æ–π ML –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    
    def __init__(self, parent):
        self.parent = parent
        self.analyzer = MLCodeAnalyzer()
        self.neural_visualizer = None
        self.training_monitor = None
        
        self.create_ui()
    
    def create_ui(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        # Notebook –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤–∫–ª–∞–¥–æ–∫ ML
        self.notebook = ttk.Notebook(self.parent)
        self.notebook.pack(fill=tk.BOTH, expand=True)
        
        # –í–∫–ª–∞–¥–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
        self.create_code_analysis_tab()
        
        # –í–∫–ª–∞–¥–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        self.create_neural_visualization_tab()
        
        # –í–∫–ª–∞–¥–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è
        self.create_training_monitor_tab()
        
        # –í–∫–ª–∞–¥–∫–∞ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
        self.create_autocomplete_tab()
        
        # –í–∫–ª–∞–¥–∫–∞ –º–æ–¥–µ–ª–∏
        self.create_model_management_tab()
    
    def create_code_analysis_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞"""
        analysis_frame = ttk.Frame(self.notebook)
        self.notebook.add(analysis_frame, text="üîç Code Analysis")
        
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        control_frame = ttk.Frame(analysis_frame)
        control_frame.pack(fill=tk.X, padx=5, pady=5)
        
        ttk.Button(control_frame, text="ü§ñ Analyze Code", command=self.analyze_current_code).pack(side=tk.LEFT, padx=2)
        ttk.Button(control_frame, text="üîÑ Retrain Model", command=self.retrain_model).pack(side=tk.LEFT, padx=2)
        ttk.Button(control_frame, text="üìä Model Stats", command=self.show_model_stats).pack(side=tk.LEFT, padx=2)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.analysis_tree = ttk.Treeview(analysis_frame, columns=("category", "severity", "confidence", "suggestion"), show="tree headings")
        self.analysis_tree.heading("#0", text="Line")
        self.analysis_tree.heading("category", text="Category")
        self.analysis_tree.heading("severity", text="Severity")
        self.analysis_tree.heading("confidence", text="Confidence")
        self.analysis_tree.heading("suggestion", text="Suggestion")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–æ–Ω–æ–∫
        self.analysis_tree.column("#0", width=60)
        self.analysis_tree.column("category", width=80)
        self.analysis_tree.column("severity", width=70)
        self.analysis_tree.column("confidence", width=80)
        self.analysis_tree.column("suggestion", width=200)
        
        # –°–∫—Ä–æ–ª–ª–±–∞—Ä
        analysis_scrollbar = ttk.Scrollbar(analysis_frame, orient=tk.VERTICAL, command=self.analysis_tree.yview)
        self.analysis_tree.config(yscrollcommand=analysis_scrollbar.set)
        
        self.analysis_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5, pady=5)
        analysis_scrollbar.pack(side=tk.RIGHT, fill=tk.Y, pady=5)
    
    def create_neural_visualization_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        viz_frame = ttk.Frame(self.notebook)
        self.notebook.add(viz_frame, text="üß† Neural Visualization")
        
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        viz_control_frame = ttk.Frame(viz_frame)
        viz_control_frame.pack(fill=tk.X, padx=5, pady=5)
        
        ttk.Button(viz_control_frame, text="üéØ Visualize Weights", command=self.visualize_weights).pack(side=tk.LEFT, padx=2)
        ttk.Button(viz_control_frame, text="‚ö° Show Activations", command=self.visualize_activations).pack(side=tk.LEFT, padx=2)
        ttk.Button(viz_control_frame, text="üîÑ Refresh", command=self.refresh_visualization).pack(side=tk.LEFT, padx=2)
        
        # Canvas –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        self.neural_canvas = tk.Frame(viz_frame)
        self.neural_canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        self.neural_visualizer = NeuralNetworkVisualizer(self.neural_canvas)
        self.neural_visualizer.set_model(self.analyzer.model)
    
    def create_training_monitor_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        training_frame = ttk.Frame(self.notebook)
        self.notebook.add(training_frame, text="üìà Training Monitor")
        
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        training_control_frame = ttk.Frame(training_frame)
        training_control_frame.pack(fill=tk.X, padx=5, pady=5)
        
        ttk.Button(training_control_frame, text="‚ñ∂Ô∏è Start Training", command=self.start_training).pack(side=tk.LEFT, padx=2)
        ttk.Button(training_control_frame, text="‚èπÔ∏è Stop Training", command=self.stop_training).pack(side=tk.LEFT, padx=2)
        ttk.Button(training_control_frame, text="üíæ Save Model", command=self.save_model).pack(side=tk.LEFT, padx=2)
        ttk.Button(training_control_frame, text="üìÅ Load Model", command=self.load_model).pack(side=tk.LEFT, padx=2)
        
        # Canvas –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        self.training_canvas = tk.Frame(training_frame)
        self.training_canvas.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        self.training_monitor = TrainingMonitor(self.training_canvas)
    
    def create_autocomplete_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è"""
        autocomplete_frame = ttk.Frame(self.notebook)
        self.notebook.add(autocomplete_frame, text="üí° Auto-complete")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        settings_frame = ttk.LabelFrame(autocomplete_frame, text="Settings")
        settings_frame.pack(fill=tk.X, padx=5, pady=5)
        
        self.autocomplete_enabled = tk.BooleanVar(value=True)
        ttk.Checkbutton(settings_frame, text="Enable ML Auto-complete", variable=self.autocomplete_enabled).pack(anchor="w", padx=5, pady=2)
        
        self.smart_suggestions = tk.BooleanVar(value=True)
        ttk.Checkbutton(settings_frame, text="Smart Context Suggestions", variable=self.smart_suggestions).pack(anchor="w", padx=5, pady=2)
        
        self.neural_suggestions = tk.BooleanVar(value=True)
        ttk.Checkbutton(settings_frame, text="Neural-specific Suggestions", variable=self.neural_suggestions).pack(anchor="w", padx=5, pady=2)
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
        test_frame = ttk.LabelFrame(autocomplete_frame, text="Test Auto-complete")
        test_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        ttk.Label(test_frame, text="Type code to test suggestions:").pack(anchor="w", padx=5, pady=2)
        
        self.test_entry = tk.Text(test_frame, height=3, font=("Consolas", 10))
        self.test_entry.pack(fill=tk.X, padx=5, pady=2)
        self.test_entry.bind('<KeyRelease>', self.on_test_text_change)
        
        ttk.Label(test_frame, text="Suggestions:").pack(anchor="w", padx=5, pady=(10, 2))
        
        self.suggestions_listbox = tk.Listbox(test_frame, height=5, font=("Consolas", 9))
        self.suggestions_listbox.pack(fill=tk.BOTH, expand=True, padx=5, pady=2)
    
    def create_model_management_tab(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏"""
        model_frame = ttk.Frame(self.notebook)
        self.notebook.add(model_frame, text="üéõÔ∏è Model Management")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        info_frame = ttk.LabelFrame(model_frame, text="Model Information")
        info_frame.pack(fill=tk.X, padx=5, pady=5)
        
        self.model_info_text = tk.Text(info_frame, height=8, state='disabled', font=("Consolas", 9))
        self.model_info_text.pack(fill=tk.X, padx=5, pady=5)
        
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        management_frame = ttk.Frame(model_frame)
        management_frame.pack(fill=tk.X, padx=5, pady=5)
        
        ttk.Button(management_frame, text="üìä Model Summary", command=self.show_model_summary).pack(side=tk.LEFT, padx=2)
        ttk.Button(management_frame, text="üîß Optimize Model", command=self.optimize_model).pack(side=tk.LEFT, padx=2)
        ttk.Button(management_frame, text="üìà Performance Test", command=self.test_model_performance).pack(side=tk.LEFT, padx=2)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        params_frame = ttk.LabelFrame(model_frame, text="Model Parameters")
        params_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # Learning Rate
        ttk.Label(params_frame, text="Learning Rate:").grid(row=0, column=0, sticky="w", padx=5, pady=2)
        self.lr_var = tk.StringVar(value="0.001")
        ttk.Entry(params_frame, textvariable=self.lr_var, width=10).grid(row=0, column=1, padx=5, pady=2)
        
        # Batch Size
        ttk.Label(params_frame, text="Batch Size:").grid(row=0, column=2, sticky="w", padx=5, pady=2)
        self.batch_var = tk.StringVar(value="32")
        ttk.Entry(params_frame, textvariable=self.batch_var, width=10).grid(row=0, column=3, padx=5, pady=2)
        
        # Hidden Dimensions
        ttk.Label(params_frame, text="Hidden Dim:").grid(row=1, column=0, sticky="w", padx=5, pady=2)
        self.hidden_var = tk.StringVar(value="256")
        ttk.Entry(params_frame, textvariable=self.hidden_var, width=10).grid(row=1, column=1, padx=5, pady=2)
        
        # Dropout Rate
        ttk.Label(params_frame, text="Dropout:").grid(row=1, column=2, sticky="w", padx=5, pady=2)
        self.dropout_var = tk.StringVar(value="0.3")
        ttk.Entry(params_frame, textvariable=self.dropout_var, width=10).grid(row=1, column=3, padx=5, pady=2)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
        self.update_model_info()
    
    # –ú–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π
    
    def analyze_current_code(self):
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–¥–∞"""
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–¥–∞ –∏–∑ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ (–∑–∞–≥–ª—É—à–∫–∞)
        sample_code = """
neuron InputNeuron {
    activation: linear
    weights: [0.5, 0.3, 0.2]
}

for i in range(len(data)):
    if result == None:
        process_data[i]

network DeepNet {
    neurons: [InputNeuron, HiddenNeuron]
    learning_rate: 0.001
}

except:
    pass
"""
        
        patterns = self.analyzer.analyze_code(sample_code)
        
        # –û—á–∏—Å—Ç–∫–∞ –¥–µ—Ä–µ–≤–∞
        for item in self.analysis_tree.get_children():
            self.analysis_tree.delete(item)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for i, pattern in enumerate(patterns, 1):
            self.analysis_tree.insert("", "end", 
                text=f"Line {i}", 
                values=(pattern.category, pattern.severity, f"{pattern.confidence:.2f}", pattern.suggestion)
            )
    
    def retrain_model(self):
        """–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        messagebox.showinfo("Retraining", "Model retraining started in background...")
        
        def retrain():
            self.analyzer.train_initial_model()
            self.parent.after(0, lambda: messagebox.showinfo("Complete", "Model retraining completed!"))
        
        threading.Thread(target=retrain, daemon=True).start()
    
    def show_model_stats(self):
        """–ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏"""
        stats = f"""Model Statistics:

Architecture: LSTM + Dense
Parameters: {sum(p.numel() for p in self.analyzer.model.parameters()):,}
Trainable: {sum(p.numel() for p in self.analyzer.model.parameters() if p.requires_grad):,}
Memory: ~{sum(p.numel() * 4 for p in self.analyzer.model.parameters()) / 1024 / 1024:.1f} MB

Training Status: {'Trained' if self.analyzer.is_trained else 'Not Trained'}
Vocabulary Size: {self.analyzer.vocab_size:,}
"""
        messagebox.showinfo("Model Statistics", stats)
    
    def visualize_weights(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤"""
        if self.neural_visualizer:
            self.neural_visualizer.visualize_weights()
    
    def visualize_activations(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π"""
        if self.neural_visualizer:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            test_input = torch.randint(0, self.analyzer.vocab_size, (1, 10))
            self.neural_visualizer.visualize_activations(test_input)
    
    def refresh_visualization(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"""
        if self.neural_visualizer:
            self.neural_visualizer.set_model(self.analyzer.model)
            self.visualize_weights()
    
    def start_training(self):
        """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
        if self.training_monitor:
            self.training_monitor.start_training_simulation()
    
    def stop_training(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if self.training_monitor:
            self.training_monitor.stop_training()
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            torch.save(self.analyzer.model.state_dict(), 'anamorph_ml_model.pth')
            messagebox.showinfo("Success", "Model saved successfully!")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to save model: {e}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            self.analyzer.model.load_state_dict(torch.load('anamorph_ml_model.pth'))
            messagebox.showinfo("Success", "Model loaded successfully!")
            self.update_model_info()
        except Exception as e:
            messagebox.showerror("Error", f"Failed to load model: {e}")
    
    def on_test_text_change(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        text = self.test_entry.get("1.0", tk.END)
        cursor_pos = len(text) - 1
        
        suggestions = self.analyzer.get_autocomplete_suggestions(text, cursor_pos)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        self.suggestions_listbox.delete(0, tk.END)
        for suggestion in suggestions:
            self.suggestions_listbox.insert(tk.END, suggestion)
    
    def show_model_summary(self):
        """–ü–æ–∫–∞–∑ —Å–≤–æ–¥–∫–∏ –º–æ–¥–µ–ª–∏"""
        summary = f"""Model Architecture Summary:

{self.analyzer.model}

Total Parameters: {sum(p.numel() for p in self.analyzer.model.parameters()):,}
Trainable Parameters: {sum(p.numel() for p in self.analyzer.model.parameters() if p.requires_grad):,}

Layer Details:
"""
        for name, module in self.analyzer.model.named_modules():
            if len(list(module.children())) == 0:  # Leaf modules
                summary += f"  {name}: {module}\n"
        
        self.update_model_info_display(summary)
    
    def optimize_model(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        messagebox.showinfo("Optimization", "Model optimization started...")
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
    
    def test_model_performance(self):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        start_time = time.time()
        
        # –¢–µ—Å—Ç –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_input = torch.randint(0, self.analyzer.vocab_size, (100, 10))
        
        with torch.no_grad():
            for _ in range(100):
                _ = self.analyzer.model(test_input)
        
        end_time = time.time()
        
        performance = f"""Performance Test Results:

Test Duration: {end_time - start_time:.3f} seconds
Throughput: {100 / (end_time - start_time):.1f} inferences/second
Average Latency: {(end_time - start_time) * 10:.1f} ms per inference

Model is {'Fast' if (end_time - start_time) < 1.0 else 'Slow'} for real-time analysis.
"""
        
        messagebox.showinfo("Performance Test", performance)
    
    def update_model_info(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        info = f"""Current Model Configuration:

Architecture: CodeAnalysisModel
- Embedding Dimension: 128
- Hidden Dimension: 256
- Number of Classes: 5
- Dropout Rate: 0.3

Training Status: {'‚úÖ Trained' if self.analyzer.is_trained else '‚ùå Not Trained'}
Vocabulary Size: {self.analyzer.vocab_size:,}

Parameters:
- Total: {sum(p.numel() for p in self.analyzer.model.parameters()):,}
- Trainable: {sum(p.numel() for p in self.analyzer.model.parameters() if p.requires_grad):,}

Memory Usage: ~{sum(p.numel() * 4 for p in self.analyzer.model.parameters()) / 1024 / 1024:.1f} MB
"""
        self.update_model_info_display(info)
    
    def update_model_info_display(self, text):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        self.model_info_text.config(state='normal')
        self.model_info_text.delete("1.0", tk.END)
        self.model_info_text.insert("1.0", text)
        self.model_info_text.config(state='disabled')

# –§—É–Ω–∫—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å IDE
def integrate_real_ml_features(ide_instance):
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö ML —Ñ—É–Ω–∫—Ü–∏–π –≤ IDE"""
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–∫–∏ ML –≤ –ø—Ä–∞–≤–æ–π –ø–∞–Ω–µ–ª–∏
    if hasattr(ide_instance, 'right_notebook'):
        ml_frame = ttk.Frame(ide_instance.right_notebook)
        ide_instance.right_notebook.add(ml_frame, text="ü§ñ Real ML")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–Ω–µ–ª–∏ ML
        ml_panel = RealMLIntegrationPanel(ml_frame)
        
        return ml_panel
    
    return None

if __name__ == "__main__":
    # –¢–µ—Å—Ç —Ä–µ–∞–ª—å–Ω–æ–π ML –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    root = tk.Tk()
    root.title("Real ML Integration Test")
    root.geometry("1000x700")
    
    ml_panel = RealMLIntegrationPanel(root)
    
    root.mainloop() 