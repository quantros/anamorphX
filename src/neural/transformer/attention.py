"""
Multi-head Attention –¥–ª—è AnamorphX

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ neuro –∏ synap.
"""

import math
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

# –ò–º–ø–æ—Ä—Ç—ã –∫–æ–º–∞–Ω–¥ AnamorphX
try:
    from ...interpreter.commands import Command, CommandResult, CommandError
    from ...interpreter.runtime import ExecutionContext
    ANAMORPHX_AVAILABLE = True
except ImportError:
    ANAMORPHX_AVAILABLE = False
    # –ó–∞–≥–ª—É—à–∫–∏
    class Command:
        def __init__(self, name, description):
            self.name = name
            self.description = description
    
    class CommandResult:
        def __init__(self, success=True, message="", data=None):
            self.success = success
            self.message = message
            self.data = data or {}
    
    class ExecutionContext:
        def __init__(self):
            self.neural_entities = {}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: PyTorch not available, using fallback attention implementation")

@dataclass
class AttentionConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Multi-head Attention"""
    d_model: int = 512
    num_heads: int = 8
    dropout: float = 0.1
    temperature: float = 1.0
    use_bias: bool = True
    attention_type: str = "scaled_dot_product"  # scaled_dot_product, additive, multiplicative

class MultiHeadAttention:
    """
    Multi-head Attention –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è AnamorphX
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è
    —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ neuro –∏ synap —è–∑—ã–∫–∞ Anamorph.
    """
    
    def __init__(self, config: AttentionConfig):
        self.config = config
        self.d_k = config.d_model // config.num_heads
        
        if self.d_k * config.num_heads != config.d_model:
            raise ValueError(f"d_model ({config.d_model}) must be divisible by num_heads ({config.num_heads})")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (fallback –±–µ–∑ PyTorch)
        if TORCH_AVAILABLE:
            self._init_torch_layers()
        else:
            self._init_fallback_weights()
    
    def _init_torch_layers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–µ–≤ —Å PyTorch"""
        self.W_q = nn.Linear(self.config.d_model, self.config.d_model, bias=self.config.use_bias)
        self.W_k = nn.Linear(self.config.d_model, self.config.d_model, bias=self.config.use_bias)
        self.W_v = nn.Linear(self.config.d_model, self.config.d_model, bias=self.config.use_bias)
        self.W_o = nn.Linear(self.config.d_model, self.config.d_model, bias=self.config.use_bias)
        self.dropout = nn.Dropout(self.config.dropout)
    
    def _init_fallback_weights(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –±–µ–∑ PyTorch (–∑–∞–≥–ª—É—à–∫–∞)"""
        import random
        
        def init_matrix(rows, cols):
            return [[random.gauss(0, 0.1) for _ in range(cols)] for _ in range(rows)]
        
        self.W_q = init_matrix(self.config.d_model, self.config.d_model)
        self.W_k = init_matrix(self.config.d_model, self.config.d_model)
        self.W_v = init_matrix(self.config.d_model, self.config.d_model)
        self.W_o = init_matrix(self.config.d_model, self.config.d_model)
    
    def forward(self, query: Any, key: Any, value: Any, mask: Optional[Any] = None) -> Tuple[Any, Dict[str, Any]]:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ Multi-head Attention
        
        Args:
            query: Query —Ç–µ–Ω–∑–æ—Ä [batch_size, seq_len, d_model]
            key: Key —Ç–µ–Ω–∑–æ—Ä [batch_size, seq_len, d_model]  
            value: Value —Ç–µ–Ω–∑–æ—Ä [batch_size, seq_len, d_model]
            mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            Tuple[output, attention_weights]
        """
        if TORCH_AVAILABLE and isinstance(query, torch.Tensor):
            return self._torch_forward(query, key, value, mask)
        else:
            return self._fallback_forward(query, key, value, mask)
    
    def _torch_forward(self, query, key, value, mask=None):
        """PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è"""
        batch_size, seq_len, d_model = query.size()
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        Q = self.W_q(query).view(batch_size, seq_len, self.config.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, seq_len, self.config.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, seq_len, self.config.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled Dot-Product Attention
        attention_output, attention_weights = self._scaled_dot_product_attention(Q, K, V, mask)
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤–æ–∫
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.config.d_model
        )
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        output = self.W_o(attention_output)
        
        return output, {"attention_weights": attention_weights}
    
    def _fallback_forward(self, query, key, value, mask=None):
        """Fallback —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ PyTorch"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        seq_len = len(query) if isinstance(query, list) else 1
        d_model = self.config.d_model
        
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
        output = query  # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        attention_weights = [[1.0 / seq_len for _ in range(seq_len)] for _ in range(seq_len)]
        
        return output, {"attention_weights": attention_weights}
    
    def _scaled_dot_product_attention(self, Q, K, V, mask=None):
        """Scaled Dot-Product Attention"""
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (math.sqrt(d_k) * self.config.temperature)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        output = torch.matmul(attention_weights, V)
        return output, attention_weights

class AttentionCommand(Command):
    """
    –ö–æ–º–∞–Ω–¥–∞ attention –¥–ª—è AnamorphX
    
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –∫ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å—É—â–Ω–æ—Å—Ç—è–º.
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ neuro –∏ synap.
    """
    
    def __init__(self):
        super().__init__("attention", "Apply multi-head attention mechanism")
        self.attention_modules = {}
    
    def execute(self, context: ExecutionContext, **kwargs) -> CommandResult:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã attention
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            source: –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö (–Ω–µ–π—Ä–æ–Ω/—Å–ª–æ–π)
            target: —Ü–µ–ª—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è
            num_heads: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 8)
            d_model: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 512)
            attention_type: —Ç–∏–ø –≤–Ω–∏–º–∞–Ω–∏—è
        """
        try:
            source = kwargs.get('source')
            target = kwargs.get('target', source)
            num_heads = int(kwargs.get('num_heads', 8))
            d_model = int(kwargs.get('d_model', 512))
            attention_type = kwargs.get('attention_type', 'scaled_dot_product')
            
            if not source:
                raise CommandError("Source entity is required for attention")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è
            config = AttentionConfig(
                d_model=d_model,
                num_heads=num_heads,
                attention_type=attention_type
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥—É–ª—è –≤–Ω–∏–º–∞–Ω–∏—è
            attention_id = f"attention_{source}_{int(time.time() * 1000)}"
            attention_module = MultiHeadAttention(config)
            self.attention_modules[attention_id] = attention_module
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if hasattr(context, 'neural_entities') and source in context.neural_entities:
                source_entity = context.neural_entities[source]
                
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
                if hasattr(source_entity, 'data'):
                    query = key = value = source_entity.data
                    output, attention_info = attention_module.forward(query, key, value)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    if target not in context.neural_entities:
                        context.neural_entities[target] = type(source_entity)(target)
                    
                    context.neural_entities[target].data = output
                    context.neural_entities[target].metadata = {
                        **getattr(context.neural_entities[target], 'metadata', {}),
                        'attention_applied': True,
                        'attention_id': attention_id,
                        'attention_config': config.__dict__,
                        'attention_info': attention_info
                    }
                    
                    return CommandResult(
                        success=True,
                        message=f"Multi-head attention applied: {source} -> {target}",
                        data={
                            "attention_id": attention_id,
                            "config": config.__dict__,
                            "num_heads": num_heads,
                            "d_model": d_model,
                            "torch_available": TORCH_AVAILABLE
                        }
                    )
                else:
                    raise CommandError(f"Source entity '{source}' has no data")
            else:
                raise CommandError(f"Source entity '{source}' not found")
                
        except Exception as e:
            return CommandResult(
                success=False,
                message=f"Attention command failed: {str(e)}",
                error=CommandError("ATTENTION_ERROR", str(e))
            )

def demo_multi_head_attention():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Multi-head Attention."""
    print("üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Multi-head Attention")
    print("=" * 50)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    batch_size, seq_len, d_model = 2, 16, 512
    num_heads = 8
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = AttentionConfig(d_model=d_model, num_heads=num_heads)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if TORCH_AVAILABLE:
        x = torch.randn(batch_size, seq_len, d_model)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (PyTorch): {x.shape}")
    else:
        x = [[0.1] * d_model for _ in range(seq_len)]
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (fallback): {len(x)}x{len(x[0])}")
    
    # –°–æ–∑–¥–∞–µ–º Multi-head Attention
    attention = MultiHeadAttention(config)
    
    print(f"‚öôÔ∏è  –†–µ–∂–∏–º: {'PyTorch' if TORCH_AVAILABLE else 'NumPy fallback'}")
    print(f"üìà –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: d_model={d_model}, num_heads={num_heads}, d_k={attention.d_k}")
    
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    import time
    start_time = time.time()
    
    output, attention_info = attention.forward(x, x, x)  # Self-attention
    
    end_time = time.time()
    
    print(f"‚úÖ Multi-head Attention –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {(end_time - start_time) * 1000:.2f}–º—Å")
    if hasattr(output, 'shape'):
        print(f"üìà –í—ã—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {output.shape}")
    else:
        print(f"üìà –í—ã—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {len(output)}x{len(output[0]) if output else 0}")
    
    return {
        'attention': attention,
        'output': output,
        'attention_info': attention_info,
        'processing_time_ms': (end_time - start_time) * 1000
    }


# –≠–∫—Å–ø–æ—Ä—Ç—ã
__all__ = [
    "MultiHeadAttention",
    "AttentionConfig", 
    "AttentionCommand",
    "TORCH_AVAILABLE"
]


if __name__ == "__main__":
    demo_multi_head_attention() 