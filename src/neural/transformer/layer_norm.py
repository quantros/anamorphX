"""
Layer Normalization –¥–ª—è AnamorphX Neural Backend Extensions

–†–µ–∞–ª–∏–∑—É–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ —Å–ª–æ—è–º –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ PyTorch, —Ç–∞–∫ –∏ fallback —Ä–µ–∂–∏–º—ã.
"""

import numpy as np
from typing import Optional, Tuple, Union
import warnings

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    warnings.warn("PyTorch –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Layer Normalization.")


class LayerNormalization:
    """
    Layer Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö.
    
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (features),
    –ø—Ä–∏–º–µ–Ω—è—è –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã gamma (scale) –∏ beta (shift).
    """
    
    def __init__(self, 
                 d_model: int = 512,
                 eps: float = 1e-6,
                 use_torch: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Layer Normalization.
        
        Args:
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
            eps: –ú–∞–ª–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            use_torch: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        self.d_model = d_model
        self.eps = eps
        self.use_torch = use_torch and TORCH_AVAILABLE
        
        if self.use_torch:
            self._init_torch_layer()
        else:
            self._init_numpy_params()
    
    def _init_torch_layer(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyTorch —Å–ª–æ—è."""
        self.layer_norm = nn.LayerNorm(self.d_model, eps=self.eps)
        
    def _init_numpy_params(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è NumPy —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏."""
        # –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.gamma = np.ones(self.d_model, dtype=np.float32)  # scale
        self.beta = np.zeros(self.d_model, dtype=np.float32)  # shift
    
    def forward(self, x: Union[np.ndarray, 'torch.Tensor']) -> Union[np.ndarray, 'torch.Tensor']:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ Layer Normalization.
        
        Args:
            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (..., d_model)
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã
        """
        if self.use_torch and TORCH_AVAILABLE:
            return self._forward_torch(x)
        else:
            return self._forward_numpy(x)
    
    def _forward_torch(self, x: 'torch.Tensor') -> 'torch.Tensor':
        """PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞."""
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, dtype=torch.float32)
        
        return self.layer_norm(x)
    
    def _forward_numpy(self, x: np.ndarray) -> np.ndarray:
        """NumPy fallback —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞."""
        if hasattr(x, 'detach'):  # –ï—Å–ª–∏ —ç—Ç–æ torch.Tensor
            x = x.detach().cpu().numpy()
        
        x = np.asarray(x, dtype=np.float32)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—é –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        mean = np.mean(x, axis=-1, keepdims=True)
        var = np.var(x, axis=-1, keepdims=True)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        x_norm = (x - mean) / np.sqrt(var + self.eps)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        return self.gamma * x_norm + self.beta
    
    def __call__(self, x: Union[np.ndarray, 'torch.Tensor']) -> Union[np.ndarray, 'torch.Tensor']:
        """–î–µ–ª–∞–µ—Ç –æ–±—ä–µ–∫—Ç –≤—ã–∑—ã–≤–∞–µ–º—ã–º."""
        return self.forward(x)


class PreNormResidualBlock:
    """
    Pre-Norm Residual Block —Å Layer Normalization.
    
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç Layer Norm –ø–µ—Ä–µ–¥ –æ—Å–Ω–æ–≤–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç residual connection.
    –≠—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –≤ GPT –∏ –¥—Ä—É–≥–∏—Ö Transformer –º–æ–¥–µ–ª—è—Ö.
    """
    
    def __init__(self, d_model: int = 512, eps: float = 1e-6):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Pre-Norm –±–ª–æ–∫–∞.
        
        Args:
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            eps: –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        """
        self.layer_norm = LayerNormalization(d_model, eps)
        self.d_model = d_model
    
    def forward(self, x: Union[np.ndarray, 'torch.Tensor'], 
                sublayer_fn: callable) -> Union[np.ndarray, 'torch.Tensor']:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ Pre-Norm –±–ª–æ–∫–∞.
        
        Args:
            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä
            sublayer_fn: –§—É–Ω–∫—Ü–∏—è –ø–æ–¥—Å–ª–æ—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, attention –∏–ª–∏ feed-forward)
            
        Returns:
            –í—ã—Ö–æ–¥ —Å residual connection: x + sublayer(LayerNorm(x))
        """
        # Pre-normalization
        normalized = self.layer_norm(x)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–¥—Å–ª–æ–π
        sublayer_output = sublayer_fn(normalized)
        
        # Residual connection
        if hasattr(x, 'shape') and hasattr(sublayer_output, 'shape'):
            if x.shape != sublayer_output.shape:
                warnings.warn(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: {x.shape} vs {sublayer_output.shape}")
        
        return x + sublayer_output


def demo_layer_normalization():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Layer Normalization."""
    print("üß† –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Layer Normalization")
    print("=" * 50)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    batch_size, seq_len, d_model = 2, 10, 512
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if TORCH_AVAILABLE:
        x = torch.randn(batch_size, seq_len, d_model)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (PyTorch): {x.shape}")
    else:
        x = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (NumPy): {x.shape}")
    
    # –°–æ–∑–¥–∞–µ–º Layer Normalization
    layer_norm = LayerNormalization(d_model=d_model)
    
    print(f"‚öôÔ∏è  –†–µ–∂–∏–º: {'PyTorch' if layer_norm.use_torch else 'NumPy fallback'}")
    
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    import time
    start_time = time.time()
    
    normalized = layer_norm(x)
    
    end_time = time.time()
    
    print(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {(end_time - start_time) * 1000:.2f}–º—Å")
    print(f"üìà –í—ã—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {normalized.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    if hasattr(normalized, 'detach'):
        stats_data = normalized.detach().cpu().numpy()
    else:
        stats_data = normalized
    
    mean = np.mean(stats_data, axis=-1)
    std = np.std(stats_data, axis=-1)
    
    print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {np.mean(mean):.6f} (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~0)")
    print(f"üìä –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.mean(std):.6f} (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å ~1)")
    
    # –¢–µ—Å—Ç Pre-Norm –±–ª–æ–∫–∞
    print("\nüîÑ –¢–µ—Å—Ç Pre-Norm Residual Block")
    print("-" * 30)
    
    pre_norm_block = PreNormResidualBlock(d_model)
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–¥—Å–ª–æ—è (identity)
    def identity_sublayer(x):
        return x * 0.1  # –ù–µ–±–æ–ª—å—à–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    
    start_time = time.time()
    residual_output = pre_norm_block.forward(x, identity_sublayer)
    end_time = time.time()
    
    print(f"‚úÖ Pre-Norm –±–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {(end_time - start_time) * 1000:.2f}–º—Å")
    print(f"üìà Residual –≤—ã—Ö–æ–¥: {residual_output.shape}")
    
    return {
        'layer_norm': layer_norm,
        'pre_norm_block': pre_norm_block,
        'input_shape': x.shape,
        'output_shape': normalized.shape,
        'processing_time_ms': (end_time - start_time) * 1000
    }


if __name__ == "__main__":
    demo_layer_normalization() 