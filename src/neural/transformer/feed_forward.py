"""
Feed-Forward Network –¥–ª—è AnamorphX Neural Backend Extensions

–†–µ–∞–ª–∏–∑—É–µ—Ç –¥–≤—É—Ö—Å–ª–æ–π–Ω—É—é feed-forward —Å–µ—Ç—å –¥–ª—è Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ PyTorch, —Ç–∞–∫ –∏ fallback —Ä–µ–∂–∏–º—ã.
"""

import numpy as np
from typing import Union
import warnings

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    warnings.warn("PyTorch –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Feed-Forward Network.")


class FeedForwardNetwork:
    """
    Feed-Forward Network –¥–ª—è Transformer –±–ª–æ–∫–∞.
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Linear(d_model ‚Üí d_ff) ‚Üí ReLU ‚Üí Dropout ‚Üí Linear(d_ff ‚Üí d_model)
    –°–ª–µ–¥—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏–∑ "Attention Is All You Need".
    """
    
    def __init__(self, 
                 d_model: int = 512,
                 d_ff: int = 2048,
                 dropout: float = 0.1,
                 activation: str = 'relu',
                 use_torch: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Feed-Forward —Å–µ—Ç–∏.
        
        Args:
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞)
            d_ff: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (–æ–±—ã—á–Ω–æ 4 * d_model)
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
            activation: –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ('relu', 'gelu', 'swish')
            use_torch: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        self.d_model = d_model
        self.d_ff = d_ff
        self.dropout = dropout
        self.activation = activation
        self.use_torch = use_torch and TORCH_AVAILABLE
        
        if self.use_torch:
            self._init_torch_layers()
        else:
            self._init_numpy_params()
    
    def _init_torch_layers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyTorch —Å–ª–æ–µ–≤."""
        self.linear1 = nn.Linear(self.d_model, self.d_ff)
        self.linear2 = nn.Linear(self.d_ff, self.d_model)
        self.dropout_layer = nn.Dropout(self.dropout)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (Xavier/Glorot)
        nn.init.xavier_uniform_(self.linear1.weight)
        nn.init.xavier_uniform_(self.linear2.weight)
        nn.init.constant_(self.linear1.bias, 0.)
        nn.init.constant_(self.linear2.bias, 0.)
        
    def _init_numpy_params(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è NumPy —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏."""
        # Xavier/Glorot –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        limit1 = np.sqrt(6.0 / (self.d_model + self.d_ff))
        limit2 = np.sqrt(6.0 / (self.d_ff + self.d_model))
        
        self.W1 = np.random.uniform(-limit1, limit1, (self.d_model, self.d_ff)).astype(np.float32)
        self.b1 = np.zeros(self.d_ff, dtype=np.float32)
        
        self.W2 = np.random.uniform(-limit2, limit2, (self.d_ff, self.d_model)).astype(np.float32)
        self.b2 = np.zeros(self.d_model, dtype=np.float32)
    
    def _apply_activation(self, x: Union[np.ndarray, 'torch.Tensor']) -> Union[np.ndarray, 'torch.Tensor']:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏."""
        if self.use_torch and isinstance(x, torch.Tensor):
            if self.activation == 'relu':
                return F.relu(x)
            elif self.activation == 'gelu':
                return F.gelu(x)
            elif self.activation == 'swish':
                return x * torch.sigmoid(x)
            else:
                return F.relu(x)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ReLU
        else:
            # NumPy —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
            if self.activation == 'relu':
                return np.maximum(0, x)
            elif self.activation == 'gelu':
                # –ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–∞—è GELU: 0.5 * x * (1 + tanh(sqrt(2/œÄ) * (x + 0.044715 * x^3)))
                return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
            elif self.activation == 'swish':
                return x * (1 / (1 + np.exp(-x)))  # x * sigmoid(x)
            else:
                return np.maximum(0, x)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ReLU
    
    def forward(self, x: Union[np.ndarray, 'torch.Tensor']) -> Union[np.ndarray, 'torch.Tensor']:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ Feed-Forward —Å–µ—Ç–∏."""
        if self.use_torch and TORCH_AVAILABLE:
            return self._forward_torch(x)
        else:
            return self._forward_numpy(x)
    
    def _forward_torch(self, x: 'torch.Tensor') -> 'torch.Tensor':
        """PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞."""
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, dtype=torch.float32)
        
        # x ‚Üí linear1 ‚Üí activation ‚Üí dropout ‚Üí linear2
        x = self.linear1(x)
        x = self._apply_activation(x)
        x = self.dropout_layer(x)
        x = self.linear2(x)
        
        return x
    
    def _forward_numpy(self, x: np.ndarray) -> np.ndarray:
        """NumPy fallback —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞."""
        if hasattr(x, 'detach'):  # –ï—Å–ª–∏ —ç—Ç–æ torch.Tensor
            x = x.detach().cpu().numpy()
        
        x = np.asarray(x, dtype=np.float32)
        
        # –ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        x = np.dot(x, self.W1) + self.b1
        
        # –ê–∫—Ç–∏–≤–∞—Ü–∏—è
        x = self._apply_activation(x)
        
        # Dropout (–≤ —Ä–µ–∂–∏–º–µ inference - –ø—Ä–æ—Å—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ)
        if self.dropout > 0:
            x = x * (1 - self.dropout)
        
        # –í—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        x = np.dot(x, self.W2) + self.b2
        
        return x
    
    def __call__(self, x: Union[np.ndarray, 'torch.Tensor']) -> Union[np.ndarray, 'torch.Tensor']:
        """–î–µ–ª–∞–µ—Ç –æ–±—ä–µ–∫—Ç –≤—ã–∑—ã–≤–∞–µ–º—ã–º."""
        return self.forward(x)


def demo_feed_forward_network():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã Feed-Forward Network."""
    print("üîß –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Feed-Forward Network")
    print("=" * 50)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    batch_size, seq_len, d_model = 2, 16, 512
    d_ff = 2048
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if TORCH_AVAILABLE:
        x = torch.randn(batch_size, seq_len, d_model)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (PyTorch): {x.shape}")
    else:
        x = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (NumPy): {x.shape}")
    
    # –¢–µ—Å—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π FFN
    print(f"\nüîß –¢–µ—Å—Ç: Feed-Forward Network")
    print("-" * 40)
    
    ffn = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, activation='relu')
    print(f"‚úÖ FFN —Å–æ–∑–¥–∞–Ω–∞ (d_model={d_model}, d_ff={d_ff})")
    print(f"   –†–µ–∂–∏–º: {'PyTorch' if ffn.use_torch else 'NumPy fallback'}")
    print(f"   –ê–∫—Ç–∏–≤–∞—Ü–∏—è: {ffn.activation}")
    
    import time
    start_time = time.time()
    output = ffn(x)
    end_time = time.time()
    
    print(f"‚úÖ Forward pass: {(end_time - start_time) * 1000:.2f}–º—Å")
    print(f"üìà –§–æ—Ä–º–∞: {x.shape} ‚Üí {output.shape}")
    
    # –¢–µ—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π
    activations = ['relu', 'gelu', 'swish']
    print(f"\nüéØ –¢–µ—Å—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π:")
    
    for activation in activations:
        ffn_act = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, activation=activation)
        
        start_time = time.time()
        output_act = ffn_act(x)
        end_time = time.time()
        
        print(f"   {activation.upper()}: {(end_time - start_time) * 1000:.2f}–º—Å")
    
    return {
        'ffn': ffn,
        'input_shape': x.shape,
        'output_shape': output.shape,
        'processing_time_ms': (end_time - start_time) * 1000
    }


if __name__ == "__main__":
    demo_feed_forward_network() 