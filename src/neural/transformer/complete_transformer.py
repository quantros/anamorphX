"""
Complete Transformer Block –¥–ª—è AnamorphX Neural Backend Extensions

–ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformer –±–ª–æ–∫–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- Multi-head Attention
- Positional Encoding  
- Layer Normalization
- Feed-Forward Network

–°–ª–µ–¥—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π Pre-Norm –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ.
"""

import sys
import os
import numpy as np
from typing import Optional, Union
import warnings

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    warnings.warn("PyTorch –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Transformer.")

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from attention import MultiHeadAttention, AttentionConfig
from positional_encoding import PositionalEncoder, PositionalEncodingConfig
from layer_norm import LayerNormalization, PreNormResidualBlock
from feed_forward import FeedForwardNetwork


class TransformerBlock:
    """
    –ü–æ–ª–Ω—ã–π Transformer –±–ª–æ–∫ —Å Pre-Norm –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π.
    
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    1. x + MultiHeadAttention(LayerNorm(x))
    2. x + FeedForward(LayerNorm(x))
    
    –≠—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è Pre-Norm –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –≤ GPT –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö.
    """
    
    def __init__(self,
                 d_model: int = 512,
                 num_heads: int = 8,
                 d_ff: int = 2048,
                 dropout: float = 0.1,
                 activation: str = 'relu',
                 max_seq_length: int = 1000,
                 use_torch: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transformer –±–ª–æ–∫–∞.
        
        Args:
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
            d_ff: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å Feed-Forward —Å–µ—Ç–∏
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
            activation: –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è FFN
            max_seq_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            use_torch: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout = dropout
        self.activation = activation
        self.use_torch = use_torch and TORCH_AVAILABLE
        
        # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._init_components()
    
    def _init_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Transformer –±–ª–æ–∫–∞."""
        # Multi-head Attention
        attention_config = AttentionConfig(
            d_model=self.d_model,
            num_heads=self.num_heads,
            dropout=self.dropout
        )
        self.attention = MultiHeadAttention(attention_config)
        
        # Feed-Forward Network
        self.feed_forward = FeedForwardNetwork(
            d_model=self.d_model,
            d_ff=self.d_ff,
            dropout=self.dropout,
            activation=self.activation,
            use_torch=self.use_torch
        )
        
        # Layer Normalization –±–ª–æ–∫–∏ (Pre-Norm)
        self.attention_norm = PreNormResidualBlock(self.d_model)
        self.ff_norm = PreNormResidualBlock(self.d_model)
        
        # Positional Encoding (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        pos_config = PositionalEncodingConfig(
            d_model=self.d_model,
            max_length=1000,
            encoding_type="sinusoidal"
        )
        self.pos_encoding = PositionalEncoder(pos_config)
    
    def forward(self, 
                x: Union[np.ndarray, 'torch.Tensor'],
                mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None,
                add_positional: bool = True) -> Union[np.ndarray, 'torch.Tensor']:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ Transformer –±–ª–æ–∫–∞.
        
        Args:
            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, d_model)
            mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            add_positional: –î–æ–±–∞–≤–ª—è—Ç—å –ª–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã
        """
        # 1. –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if add_positional:
            x = self.pos_encoding(x)
        
        # 2. Self-attention —Å Pre-Norm –∏ residual connection
        def attention_sublayer(normalized_x):
            output, _ = self.attention.forward(normalized_x, normalized_x, normalized_x, mask)
            return output
        
        x = self.attention_norm.forward(x, attention_sublayer)
        
        # 3. Feed-forward —Å Pre-Norm –∏ residual connection
        x = self.ff_norm.forward(x, self.feed_forward)
        
        return x
    
    def __call__(self, 
                 x: Union[np.ndarray, 'torch.Tensor'],
                 mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None,
                 add_positional: bool = True) -> Union[np.ndarray, 'torch.Tensor']:
        """–î–µ–ª–∞–µ—Ç –æ–±—ä–µ–∫—Ç –≤—ã–∑—ã–≤–∞–µ–º—ã–º."""
        return self.forward(x, mask, add_positional)


class TransformerEncoder:
    """
    –ü–æ–ª–Ω—ã–π Transformer Encoder –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–ª–æ–∫–æ–≤.
    
    –°—Ç–µ–∫ –∏–∑ N Transformer –±–ª–æ–∫–æ–≤ —Å —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π.
    """
    
    def __init__(self,
                 num_layers: int = 6,
                 d_model: int = 512,
                 num_heads: int = 8,
                 d_ff: int = 2048,
                 dropout: float = 0.1,
                 activation: str = 'relu',
                 max_seq_length: int = 1000,
                 use_torch: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transformer Encoder.
        
        Args:
            num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Transformer –±–ª–æ–∫–æ–≤
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
            d_ff: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å Feed-Forward —Å–µ—Ç–∏
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
            activation: –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
            max_seq_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            use_torch: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        self.num_layers = num_layers
        self.d_model = d_model
        self.use_torch = use_torch and TORCH_AVAILABLE
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç–µ–∫ Transformer –±–ª–æ–∫–æ–≤
        self.layers = []
        for i in range(num_layers):
            layer = TransformerBlock(
                d_model=d_model,
                num_heads=num_heads,
                d_ff=d_ff,
                dropout=dropout,
                activation=activation,
                max_seq_length=max_seq_length,
                use_torch=use_torch
            )
            self.layers.append(layer)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        self.final_norm = LayerNormalization(d_model, use_torch=use_torch)
    
    def forward(self, 
                x: Union[np.ndarray, 'torch.Tensor'],
                mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None) -> Union[np.ndarray, 'torch.Tensor']:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ Encoder.
        
        Args:
            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, d_model)
            mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        """
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏
        for i, layer in enumerate(self.layers):
            # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ
            add_pos = (i == 0)
            x = layer(x, mask, add_positional=add_pos)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        x = self.final_norm(x)
        
        return x
    
    def __call__(self, 
                 x: Union[np.ndarray, 'torch.Tensor'],
                 mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None) -> Union[np.ndarray, 'torch.Tensor']:
        """–î–µ–ª–∞–µ—Ç –æ–±—ä–µ–∫—Ç –≤—ã–∑—ã–≤–∞–µ–º—ã–º."""
        return self.forward(x, mask)


def demo_complete_transformer():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–æ–ª–Ω–æ–≥–æ Transformer –±–ª–æ–∫–∞."""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Complete Transformer Block")
    print("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    batch_size, seq_len, d_model = 2, 16, 512
    num_heads = 8
    d_ff = 2048
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if TORCH_AVAILABLE:
        x = torch.randn(batch_size, seq_len, d_model)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (PyTorch): {x.shape}")
    else:
        x = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (NumPy): {x.shape}")
    
    # –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ Transformer –±–ª–æ–∫–∞
    print("\nüß† –¢–µ—Å—Ç 1: –û–¥–∏–Ω–æ—á–Ω—ã–π Transformer Block")
    print("-" * 50)
    
    transformer_block = TransformerBlock(
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff,
        dropout=0.1,
        activation='relu'
    )
    
    print(f"‚úÖ TransformerBlock —Å–æ–∑–¥–∞–Ω")
    print(f"   –†–µ–∂–∏–º: {'PyTorch' if transformer_block.use_torch else 'NumPy fallback'}")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: d_model={d_model}, heads={num_heads}, d_ff={d_ff}")
    
    import time
    start_time = time.time()
    
    output = transformer_block(x)
    
    end_time = time.time()
    
    print(f"‚úÖ Transformer –±–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {(end_time - start_time) * 1000:.2f}–º—Å")
    print(f"üìà –§–æ—Ä–º–∞: {x.shape} ‚Üí {output.shape}")
    print(f"   Residual connections: {'‚úÖ –†–∞–±–æ—Ç–∞—é—Ç' if output.shape == x.shape else '‚ùå –û—à–∏–±–∫–∞'}")
    
    # –¢–µ—Å—Ç Transformer Encoder (—Å—Ç–µ–∫ –±–ª–æ–∫–æ–≤)
    print("\nüèóÔ∏è  –¢–µ—Å—Ç 2: Transformer Encoder (3 —Å–ª–æ—è)")
    print("-" * 50)
    
    encoder = TransformerEncoder(
        num_layers=3,
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff,
        dropout=0.1,
        activation='relu'
    )
    
    print(f"‚úÖ TransformerEncoder —Å–æ–∑–¥–∞–Ω ({encoder.num_layers} —Å–ª–æ–µ–≤)")
    
    start_time = time.time()
    
    encoded = encoder(x)
    
    end_time = time.time()
    
    print(f"‚úÖ Transformer Encoder –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {(end_time - start_time) * 1000:.2f}–º—Å")
    print(f"üìà –§–æ—Ä–º–∞: {x.shape} ‚Üí {encoded.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–æ—Ä–º–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞—Å—å
    input_shape = x.shape
    output_shape = encoded.shape if hasattr(encoded, 'shape') else np.array(encoded).shape
    
    print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º—ã: {input_shape} ‚Üí {output_shape}")
    print(f"‚úÖ –§–æ—Ä–º–∞ {'—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞' if input_shape == output_shape else '–∏–∑–º–µ–Ω–µ–Ω–∞'}")
    
    # –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    print("\n‚ö° –¢–µ—Å—Ç 3: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
    print("-" * 50)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
    components_time = {}
    
    # Attention
    start_time = time.time()
    attn_output, _ = transformer_block.attention.forward(x, x, x)
    components_time['attention'] = (time.time() - start_time) * 1000
    
    # Feed-Forward
    start_time = time.time()
    ff_output = transformer_block.feed_forward(x)
    components_time['feed_forward'] = (time.time() - start_time) * 1000
    
    # Layer Norm
    start_time = time.time()
    norm_output = transformer_block.attention_norm.layer_norm(x)
    components_time['layer_norm'] = (time.time() - start_time) * 1000
    
    # Positional Encoding
    start_time = time.time()
    pos_output = transformer_block.pos_encoding(x)
    components_time['positional'] = (time.time() - start_time) * 1000
    
    print("üìä –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:")
    for component, time_ms in components_time.items():
        print(f"   {component.replace('_', ' ').title()}: {time_ms:.2f}–º—Å")
    
    total_component_time = sum(components_time.values())
    full_block_time = (end_time - start_time) * 1000
    
    print(f"\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ:")
    print(f"   –°—É–º–º–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {total_component_time:.2f}–º—Å")
    print(f"   –ü–æ–ª–Ω—ã–π –±–ª–æ–∫: {full_block_time:.2f}–º—Å")
    print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {(total_component_time / full_block_time * 100):.1f}%")
    
    return {
        'transformer_block': transformer_block,
        'encoder': encoder,
        'input_shape': input_shape,
        'output_shape': output_shape,
        'single_block_time_ms': (end_time - start_time) * 1000,
        'encoder_time_ms': (end_time - start_time) * 1000,
        'components_time': components_time
    }


if __name__ == "__main__":
    demo_complete_transformer() 