"""
–ü–æ–ª–Ω—ã–π Transformer Block –¥–ª—è AnamorphX Neural Backend Extensions

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç Multi-head Attention, Positional Encoding –∏ Layer Normalization
–≤ –µ–¥–∏–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –±–ª–æ–∫ Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.
"""

import numpy as np
from typing import Optional, Union, Tuple
import warnings

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from .attention import MultiHeadAttention, AttentionConfig
from .positional_encoding import PositionalEncoder, PositionalEncodingConfig
from .layer_norm import LayerNormalization, PreNormResidualBlock


class FeedForwardNetwork:
    """
    Feed-Forward Network –¥–ª—è Transformer –±–ª–æ–∫–∞.
    
    –°–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ —Å ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π –º–µ–∂–¥—É –Ω–∏–º–∏.
    """
    
    def __init__(self, 
                 d_model: int = 512,
                 d_ff: int = 2048,
                 dropout: float = 0.1,
                 use_torch: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Feed-Forward —Å–µ—Ç–∏.
        
        Args:
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            d_ff: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è (–æ–±—ã—á–Ω–æ 4 * d_model)
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
            use_torch: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        self.d_model = d_model
        self.d_ff = d_ff
        self.dropout = dropout
        self.use_torch = use_torch and TORCH_AVAILABLE
        
        if self.use_torch:
            self._init_torch_layers()
        else:
            self._init_numpy_params()
    
    def _init_torch_layers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è PyTorch —Å–ª–æ–µ–≤."""
        self.linear1 = nn.Linear(self.d_model, self.d_ff)
        self.linear2 = nn.Linear(self.d_ff, self.d_model)
        self.dropout_layer = nn.Dropout(self.dropout)
        
    def _init_numpy_params(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è NumPy —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏."""
        # Xavier/Glorot –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        limit1 = np.sqrt(6.0 / (self.d_model + self.d_ff))
        limit2 = np.sqrt(6.0 / (self.d_ff + self.d_model))
        
        self.W1 = np.random.uniform(-limit1, limit1, (self.d_model, self.d_ff)).astype(np.float32)
        self.b1 = np.zeros(self.d_ff, dtype=np.float32)
        
        self.W2 = np.random.uniform(-limit2, limit2, (self.d_ff, self.d_model)).astype(np.float32)
        self.b2 = np.zeros(self.d_model, dtype=np.float32)
    
    def forward(self, x: Union[np.ndarray, 'torch.Tensor']) -> Union[np.ndarray, 'torch.Tensor']:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ Feed-Forward —Å–µ—Ç–∏."""
        if self.use_torch and TORCH_AVAILABLE:
            return self._forward_torch(x)
        else:
            return self._forward_numpy(x)
    
    def _forward_torch(self, x: 'torch.Tensor') -> 'torch.Tensor':
        """PyTorch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è."""
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, dtype=torch.float32)
        
        # x -> linear1 -> ReLU -> dropout -> linear2
        x = self.linear1(x)
        x = F.relu(x)
        x = self.dropout_layer(x)
        x = self.linear2(x)
        
        return x
    
    def _forward_numpy(self, x: np.ndarray) -> np.ndarray:
        """NumPy fallback —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è."""
        if hasattr(x, 'detach'):
            x = x.detach().cpu().numpy()
        
        x = np.asarray(x, dtype=np.float32)
        
        # –ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π + ReLU
        x = np.dot(x, self.W1) + self.b1
        x = np.maximum(0, x)  # ReLU
        
        # –ü—Ä–æ—Å—Ç–æ–π dropout (–≤ —Ä–µ–∂–∏–º–µ inference)
        if self.dropout > 0:
            x = x * (1 - self.dropout)
        
        # –í—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        x = np.dot(x, self.W2) + self.b2
        
        return x
    
    def __call__(self, x: Union[np.ndarray, 'torch.Tensor']) -> Union[np.ndarray, 'torch.Tensor']:
        """–î–µ–ª–∞–µ—Ç –æ–±—ä–µ–∫—Ç –≤—ã–∑—ã–≤–∞–µ–º—ã–º."""
        return self.forward(x)


class TransformerBlock:
    """
    –ü–æ–ª–Ω—ã–π Transformer –±–ª–æ–∫.
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç Multi-head Attention, Feed-Forward Network –∏ Layer Normalization
    —Å residual connections –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π Pre-Norm –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ.
    """
    
    def __init__(self,
                 d_model: int = 512,
                 num_heads: int = 8,
                 d_ff: int = 2048,
                 dropout: float = 0.1,
                 max_seq_length: int = 1000,
                 use_torch: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transformer –±–ª–æ–∫–∞.
        
        Args:
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
            d_ff: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å Feed-Forward —Å–µ—Ç–∏
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
            max_seq_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            use_torch: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout = dropout
        self.use_torch = use_torch and TORCH_AVAILABLE
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        attention_config = AttentionConfig(
            d_model=d_model,
            num_heads=num_heads,
            dropout=dropout
        )
        
        pos_config = PositionalEncodingConfig(
            d_model=d_model,
            max_length=max_seq_length,
            dropout=dropout
        )
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –±–ª–æ–∫–∞
        self.attention = MultiHeadAttention(attention_config)
        
        self.feed_forward = FeedForwardNetwork(
            d_model=d_model,
            d_ff=d_ff,
            dropout=dropout,
            use_torch=use_torch
        )
        
        # Pre-Norm –±–ª–æ–∫–∏
        self.attention_norm = PreNormResidualBlock(d_model)
        self.ff_norm = PreNormResidualBlock(d_model)
        
        # Positional Encoding (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.pos_encoding = PositionalEncoder(pos_config)
    
    def forward(self, 
                x: Union[np.ndarray, 'torch.Tensor'],
                mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None,
                add_positional: bool = True) -> Union[np.ndarray, 'torch.Tensor']:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ Transformer –±–ª–æ–∫–∞.
        
        Args:
            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, d_model)
            mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            add_positional: –î–æ–±–∞–≤–ª—è—Ç—å –ª–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã
        """
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if add_positional:
            x, _ = self.pos_encoding.forward(x)
        
        # Self-attention —Å Pre-Norm –∏ residual connection
        def attention_sublayer(normalized_x):
            output, _ = self.attention.forward(normalized_x, normalized_x, normalized_x, mask)
            return output
        
        x = self.attention_norm.forward(x, attention_sublayer)
        
        # Feed-forward —Å Pre-Norm –∏ residual connection
        x = self.ff_norm.forward(x, self.feed_forward)
        
        return x
    
    def __call__(self, 
                 x: Union[np.ndarray, 'torch.Tensor'],
                 mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None,
                 add_positional: bool = True) -> Union[np.ndarray, 'torch.Tensor']:
        """–î–µ–ª–∞–µ—Ç –æ–±—ä–µ–∫—Ç –≤—ã–∑—ã–≤–∞–µ–º—ã–º."""
        return self.forward(x, mask, add_positional)


class TransformerEncoder:
    """
    –ü–æ–ª–Ω—ã–π Transformer Encoder –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–ª–æ–∫–æ–≤.
    """
    
    def __init__(self,
                 num_layers: int = 6,
                 d_model: int = 512,
                 num_heads: int = 8,
                 d_ff: int = 2048,
                 dropout: float = 0.1,
                 max_seq_length: int = 1000,
                 use_torch: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transformer Encoder.
        
        Args:
            num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ Transformer –±–ª–æ–∫–æ–≤
            d_model: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
            d_ff: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å Feed-Forward —Å–µ—Ç–∏
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
            max_seq_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            use_torch: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        """
        self.num_layers = num_layers
        self.d_model = d_model
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç–µ–∫ Transformer –±–ª–æ–∫–æ–≤
        self.layers = []
        for i in range(num_layers):
            layer = TransformerBlock(
                d_model=d_model,
                num_heads=num_heads,
                d_ff=d_ff,
                dropout=dropout,
                max_seq_length=max_seq_length,
                use_torch=use_torch
            )
            self.layers.append(layer)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        self.final_norm = LayerNormalization(d_model, use_torch=use_torch)
    
    def forward(self, 
                x: Union[np.ndarray, 'torch.Tensor'],
                mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None) -> Union[np.ndarray, 'torch.Tensor']:
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ Encoder.
        
        Args:
            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, d_model)
            mask: –ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        """
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏
        for i, layer in enumerate(self.layers):
            # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ
            add_pos = (i == 0)
            x = layer(x, mask, add_positional=add_pos)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        x = self.final_norm(x)
        
        return x
    
    def __call__(self, 
                 x: Union[np.ndarray, 'torch.Tensor'],
                 mask: Optional[Union[np.ndarray, 'torch.Tensor']] = None) -> Union[np.ndarray, 'torch.Tensor']:
        """–î–µ–ª–∞–µ—Ç –æ–±—ä–µ–∫—Ç –≤—ã–∑—ã–≤–∞–µ–º—ã–º."""
        return self.forward(x, mask)


def demo_transformer_block():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–æ–ª–Ω–æ–≥–æ Transformer –±–ª–æ–∫–∞."""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ Transformer –±–ª–æ–∫–∞")
    print("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    batch_size, seq_len, d_model = 2, 16, 512
    num_heads = 8
    d_ff = 2048
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    if TORCH_AVAILABLE:
        x = torch.randn(batch_size, seq_len, d_model)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (PyTorch): {x.shape}")
    else:
        x = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (NumPy): {x.shape}")
    
    # –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ Transformer –±–ª–æ–∫–∞
    print("\nüß† –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ Transformer –±–ª–æ–∫–∞")
    print("-" * 40)
    
    transformer_block = TransformerBlock(
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff,
        dropout=0.1
    )
    
    print(f"‚öôÔ∏è  –†–µ–∂–∏–º: {'PyTorch' if transformer_block.use_torch else 'NumPy fallback'}")
    
    import time
    start_time = time.time()
    
    output = transformer_block(x)
    
    end_time = time.time()
    
    print(f"‚úÖ Transformer –±–ª–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {(end_time - start_time) * 1000:.2f}–º—Å")
    print(f"üìà –í—ã—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {output.shape}")
    
    # –¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ Encoder
    print("\nüèóÔ∏è  –¢–µ—Å—Ç Transformer Encoder (3 —Å–ª–æ—è)")
    print("-" * 40)
    
    encoder = TransformerEncoder(
        num_layers=3,
        d_model=d_model,
        num_heads=num_heads,
        d_ff=d_ff,
        dropout=0.1
    )
    
    start_time = time.time()
    
    encoded = encoder(x)
    
    end_time = time.time()
    
    print(f"‚úÖ Transformer Encoder –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {(end_time - start_time) * 1000:.2f}–º—Å")
    print(f"üìà –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞: {encoded.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–æ—Ä–º–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞—Å—å
    input_shape = x.shape
    output_shape = encoded.shape if hasattr(encoded, 'shape') else np.array(encoded).shape
    
    print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º—ã: {input_shape} -> {output_shape}")
    print(f"‚úÖ –§–æ—Ä–º–∞ {'—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞' if input_shape == output_shape else '–∏–∑–º–µ–Ω–µ–Ω–∞'}")
    
    return {
        'transformer_block': transformer_block,
        'encoder': encoder,
        'input_shape': input_shape,
        'output_shape': output_shape,
        'single_block_time_ms': (end_time - start_time) * 1000,
        'encoder_time_ms': (end_time - start_time) * 1000
    }


if __name__ == "__main__":
    demo_transformer_block() 