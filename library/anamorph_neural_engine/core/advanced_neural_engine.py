"""
üß† Advanced Neural Engine –¥–ª—è AnamorphX Enterprise
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è enterprise —É—Ä–æ–≤–Ω—è
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import asyncio
import threading
import time
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from enum import Enum

class ModelType(Enum):
    """–¢–∏–ø—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    TRANSFORMER = "transformer"
    LSTM = "lstm"
    GRU = "gru"
    CNN = "cnn"
    RESNET = "resnet"
    BERT = "bert"
    GPT = "gpt"

@dataclass
class TrainingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    epochs: int = 100
    batch_size: int = 32
    learning_rate: float = 0.001
    weight_decay: float = 1e-5
    scheduler: str = "cosine"
    early_stopping: bool = True
    patience: int = 10
    validation_split: float = 0.2

class TransformerModel(nn.Module):
    """Transformer –º–æ–¥–µ–ª—å –¥–ª—è enterprise –∑–∞–¥–∞—á"""
    
    def __init__(self, vocab_size: int, d_model: int = 512, nhead: int = 8, 
                 num_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, dropout)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.classifier = nn.Linear(d_model, vocab_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        x = self.embedding(x) * np.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.transformer(x)
        x = self.classifier(x)
        return x

class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Transformer"""
    
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class LSTMModel(nn.Module):
    """LSTM –º–æ–¥–µ–ª—å —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 256, 
                 hidden_dim: int = 512, num_layers: int = 3, 
                 dropout: float = 0.3, bidirectional: bool = True):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, num_layers,
            batch_first=True, dropout=dropout, bidirectional=bidirectional
        )
        
        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim
        self.attention = AttentionLayer(lstm_output_dim)
        self.classifier = nn.Linear(lstm_output_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        attended = self.attention(lstm_out)
        output = self.classifier(self.dropout(attended))
        return output

class AttentionLayer(nn.Module):
    """–°–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è"""
    
    def __init__(self, hidden_dim: int):
        super().__init__()
        self.attention = nn.Linear(hidden_dim, 1)
        
    def forward(self, lstm_output):
        attention_weights = F.softmax(self.attention(lstm_output), dim=1)
        attended = torch.sum(attention_weights * lstm_output, dim=1)
        return attended

class CNNModel(nn.Module):
    """CNN –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 256, 
                 num_filters: int = 100, filter_sizes: List[int] = [3, 4, 5],
                 dropout: float = 0.3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.convs = nn.ModuleList([
            nn.Conv1d(embedding_dim, num_filters, kernel_size=size)
            for size in filter_sizes
        ])
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(len(filter_sizes) * num_filters, vocab_size)
        
    def forward(self, x):
        x = self.embedding(x).transpose(1, 2)  # (batch, embedding_dim, seq_len)
        conv_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(x))
            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
            conv_outputs.append(pooled)
        
        concatenated = torch.cat(conv_outputs, dim=1)
        output = self.classifier(self.dropout(concatenated))
        return output

class AdvancedNeuralEngine:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –Ω–µ–π—Ä–æ–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫"""
    
    def __init__(self, device: str = "auto", max_workers: int = 4):
        self.device = self._setup_device(device)
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # –ú–æ–¥–µ–ª–∏
        self.models: Dict[str, nn.Module] = {}
        self.optimizers: Dict[str, optim.Optimizer] = {}
        self.schedulers: Dict[str, Any] = {}
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.training_metrics: Dict[str, List[float]] = {}
        self.inference_stats: Dict[str, Any] = {}
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.model_configs: Dict[str, Dict] = {}
        
        self.logger = logging.getLogger(__name__)
        
        print(f"üß† Advanced Neural Engine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print(f"   ‚ö° Device: {self.device}")
        print(f"   üîß Max Workers: {max_workers}")
    
    def _setup_device(self, device: str) -> torch.device:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"
        
        torch_device = torch.device(device)
        print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {torch_device}")
        return torch_device
    
    async def create_model(self, model_name: str, model_type: ModelType, 
                          config: Dict[str, Any]) -> bool:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            if model_type == ModelType.TRANSFORMER:
                model = TransformerModel(
                    vocab_size=config.get('vocab_size', 10000),
                    d_model=config.get('d_model', 512),
                    nhead=config.get('nhead', 8),
                    num_layers=config.get('num_layers', 6),
                    dim_feedforward=config.get('dim_feedforward', 2048),
                    dropout=config.get('dropout', 0.1)
                )
            else:
                # –ü—Ä–æ—Å—Ç–∞—è LSTM –º–æ–¥–µ–ª—å –∫–∞–∫ fallback
                class SimpleLSTM(nn.Module):
                    def __init__(self, vocab_size, hidden_dim=256):
                        super().__init__()
                        self.embedding = nn.Embedding(vocab_size, hidden_dim)
                        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
                        self.classifier = nn.Linear(hidden_dim, vocab_size)
                    
                    def forward(self, x):
                        x = self.embedding(x)
                        x, _ = self.lstm(x)
                        x = self.classifier(x[:, -1, :])
                        return x
                
                model = SimpleLSTM(config.get('vocab_size', 10000))
            
            model = model.to(self.device)
            self.models[model_name] = model
            self.model_configs[model_name] = config
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            optimizer = optim.AdamW(
                model.parameters(),
                lr=config.get('learning_rate', 0.001),
                weight_decay=config.get('weight_decay', 1e-5)
            )
            self.optimizers[model_name] = optimizer
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —Å–æ–∑–¥–∞–Ω–∞")
            print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    async def train_model(self, model_name: str, train_data: List[Any], 
                         training_config: TrainingConfig) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            raise ValueError(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        model = self.models[model_name]
        optimizer = self.optimizers[model_name]
        scheduler = self.schedulers[model_name]
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor, self._train_model_sync, 
            model, optimizer, scheduler, train_data, training_config
        )
        
        return result
    
    def _train_model_sync(self, model: nn.Module, optimizer: optim.Optimizer,
                         scheduler: Any, train_data: List[Any], 
                         config: TrainingConfig) -> Dict[str, Any]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model.train()
        losses = []
        best_loss = float('inf')
        patience_counter = 0
        
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è (epochs: {config.epochs})")
        
        for epoch in range(config.epochs):
            epoch_losses = []
            
            # –û–±—É—á–µ–Ω–∏–µ –ø–æ –±–∞—Ç—á–∞–º
            for batch_idx in range(0, len(train_data), config.batch_size):
                batch = train_data[batch_idx:batch_idx + config.batch_size]
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–µ–Ω–∑–æ—Ä—ã
                inputs, targets = self._prepare_batch(batch)
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = F.cross_entropy(outputs, targets)
                loss.backward()
                optimizer.step()
                
                epoch_losses.append(loss.item())
            
            avg_loss = np.mean(epoch_losses)
            losses.append(avg_loss)
            scheduler.step()
            
            # Early stopping
            if config.early_stopping:
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= config.patience:
                        print(f"‚èπÔ∏è Early stopping –Ω–∞ epoch {epoch}")
                        break
            
            if epoch % 10 == 0:
                print(f"üìà Epoch {epoch}: Loss = {avg_loss:.4f}")
        
        return {
            'final_loss': losses[-1] if losses else 0,
            'best_loss': best_loss,
            'total_epochs': len(losses),
            'losses': losses
        }
    
    def _prepare_batch(self, batch: List[Any]) -> Tuple[torch.Tensor, torch.Tensor]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        if isinstance(batch[0], str):
            # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = []
            targets = []
            
            for item in batch:
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏
                tokens = [hash(word) % 10000 for word in item.split()]
                if len(tokens) < 50:
                    tokens.extend([0] * (50 - len(tokens)))
                else:
                    tokens = tokens[:50]
                
                inputs.append(tokens)
                targets.append(len(tokens) % 10)  # –ü—Ä–æ—Å—Ç–∞—è —Ü–µ–ª—å
            
            return torch.LongTensor(inputs), torch.LongTensor(targets)
        else:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
            inputs = torch.FloatTensor(batch)
            targets = torch.LongTensor([0] * len(batch))
            return inputs, targets
    
    async def predict(self, model_name: str, input_data: Any) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        if model_name not in self.models:
            raise ValueError(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        model = self.models[model_name]
        
        # –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor, self._predict_sync, model, input_data
        )
        
        return result
    
    def _predict_sync(self, model: nn.Module, input_data: Any) -> Dict[str, Any]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        model.eval()
        start_time = time.time()
        
        with torch.no_grad():
            # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if isinstance(input_data, str):
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏
                tokens = [hash(word) % 10000 for word in input_data.split()]
                if len(tokens) < 50:
                    tokens.extend([0] * (50 - len(tokens)))
                else:
                    tokens = tokens[:50]
                inputs = torch.LongTensor([tokens]).to(self.device)
            else:
                inputs = torch.FloatTensor([[input_data]]).to(self.device)
            
            outputs = model(inputs)
            probabilities = F.softmax(outputs, dim=-1)
            prediction = torch.argmax(probabilities, dim=-1)
            confidence = torch.max(probabilities).item()
        
        processing_time = time.time() - start_time
        
        return {
            'prediction': prediction.item(),
            'confidence': confidence,
            'probabilities': probabilities.cpu().numpy().tolist(),
            'processing_time': processing_time
        }
    
    async def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            return {'error': f'–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}
        
        model = self.models[model_name]
        config = self.model_configs[model_name]
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            'name': model_name,
            'type': model.__class__.__name__,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'device': str(next(model.parameters()).device),
            'config': config,
            'model_size_mb': total_params * 4 / (1024 * 1024)  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        }
    
    async def save_model(self, model_name: str, path: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            return False
        
        try:
            model = self.models[model_name]
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': self.model_configs[model_name],
                'model_type': model.__class__.__name__
            }, path)
            
            print(f"üíæ –ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    async def load_model(self, model_name: str, path: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            checkpoint = torch.load(path, map_location=self.device)
            config = checkpoint['config']
            model_type = checkpoint['model_type']
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ç–∏–ø–∞
            if model_type == 'TransformerModel':
                model = TransformerModel(**config)
            elif model_type == 'LSTMModel':
                model = LSTMModel(**config)
            elif model_type == 'CNNModel':
                model = CNNModel(**config)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
            
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(self.device)
            
            self.models[model_name] = model
            self.model_configs[model_name] = config
            
            print(f"üìÅ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {path}")
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        models_info = []
        
        for name, model in self.models.items():
            total_params = sum(p.numel() for p in model.parameters())
            models_info.append({
                'name': name,
                'type': model.__class__.__name__,
                'parameters': total_params,
                'device': str(next(model.parameters()).device)
            })
        
        return models_info
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return {
            'total_models': len(self.models),
            'device': str(self.device),
            'max_workers': self.max_workers,
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'memory_allocated': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0,
            'memory_cached': torch.cuda.memory_reserved() if torch.cuda.is_available() else 0
        }
    
    async def optimize_model(self, model_name: str) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            return {'error': f'–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}
        
        model = self.models[model_name]
        
        # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
        try:
            if hasattr(torch.quantization, 'quantize_dynamic'):
                quantized_model = torch.quantization.quantize_dynamic(
                    model, {nn.Linear}, dtype=torch.qint8
                )
                
                original_size = sum(p.numel() for p in model.parameters()) * 4
                quantized_size = sum(p.numel() for p in quantized_model.parameters()) * 1  # –ü—Ä–∏–º–µ—Ä–Ω–æ
                
                self.models[f"{model_name}_quantized"] = quantized_model
                
                return {
                    'original_size_mb': original_size / (1024 * 1024),
                    'quantized_size_mb': quantized_size / (1024 * 1024),
                    'compression_ratio': original_size / quantized_size,
                    'quantized_model_name': f"{model_name}_quantized"
                }
            else:
                return {'info': '–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è'}
                
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}'}
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.executor.shutdown(wait=True)
        
        # –û—á–∏—Å—Ç–∫–∞ CUDA –∫—ç—à–∞
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("üßπ –†–µ—Å—É—Ä—Å—ã Neural Engine –æ—á–∏—â–µ–Ω—ã") 