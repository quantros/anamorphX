"""
ü§ñ AI Optimization –¥–ª—è AnamorphX Enterprise
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time
import json
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
import psutil
import gc
from concurrent.futures import ThreadPoolExecutor

class OptimizationType(Enum):
    """–¢–∏–ø—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    QUANTIZATION = "quantization"
    PRUNING = "pruning"
    DISTILLATION = "distillation"
    ARCHITECTURE_SEARCH = "architecture_search"
    HYPERPARAMETER_TUNING = "hyperparameter_tuning"
    BATCH_SIZE_OPTIMIZATION = "batch_size_optimization"
    MEMORY_OPTIMIZATION = "memory_optimization"

@dataclass
class OptimizationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    optimization_type: str
    original_metrics: Dict[str, float]
    optimized_metrics: Dict[str, float]
    improvement_ratio: float
    optimization_time: float
    success: bool
    details: Dict[str, Any]

class ModelProfiler:
    """–ü—Ä–æ—Ñ–∞–π–ª–µ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def profile_model(self, model: nn.Module, input_shape: Tuple[int, ...], 
                          device: torch.device, num_runs: int = 100) -> Dict[str, Any]:
        """–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model.eval()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_input = torch.randn(1, *input_shape).to(device)
        
        # Warm-up
        for _ in range(10):
            with torch.no_grad():
                _ = model(test_input)
        
        torch.cuda.synchronize() if device.type == 'cuda' else None
        
        # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        start_time = time.time()
        
        for _ in range(num_runs):
            with torch.no_grad():
                output = model(test_input)
        
        torch.cuda.synchronize() if device.type == 'cuda' else None
        end_time = time.time()
        
        avg_inference_time = (end_time - start_time) / num_runs
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
        model_size_mb = total_params * 4 / (1024 * 1024)  # float32
        
        # –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        layer_info = self._analyze_layers(model)
        
        # Memory usage
        memory_usage = self._measure_memory_usage(model, test_input, device)
        
        return {
            'inference_time_ms': avg_inference_time * 1000,
            'fps': 1 / avg_inference_time,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': model_size_mb,
            'memory_usage_mb': memory_usage,
            'layer_analysis': layer_info,
            'flops': self._estimate_flops(model, input_shape)
        }
    
    def _analyze_layers(self, model: nn.Module) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏"""
        layer_types = {}
        layer_params = {}
        
        for name, module in model.named_modules():
            layer_type = type(module).__name__
            if layer_type not in layer_types:
                layer_types[layer_type] = 0
            layer_types[layer_type] += 1
            
            params = sum(p.numel() for p in module.parameters())
            if params > 0:
                layer_params[name] = params
        
        return {
            'layer_types': layer_types,
            'layer_parameters': layer_params,
            'total_layers': len(list(model.modules())) - 1  # Exclude root module
        }
    
    def _measure_memory_usage(self, model: nn.Module, test_input: torch.Tensor, 
                            device: torch.device) -> float:
        """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            with torch.no_grad():
                _ = model(test_input)
            
            peak_memory = torch.cuda.max_memory_allocated()
            memory_usage = (peak_memory - initial_memory) / (1024 * 1024)
            torch.cuda.reset_peak_memory_stats()
            return memory_usage
        else:
            # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º psutil
            process = psutil.Process()
            initial_memory = process.memory_info().rss
            
            with torch.no_grad():
                _ = model(test_input)
            
            final_memory = process.memory_info().rss
            return (final_memory - initial_memory) / (1024 * 1024)
    
    def _estimate_flops(self, model: nn.Module, input_shape: Tuple[int, ...]) -> int:
        """–û—Ü–µ–Ω–∫–∞ FLOPS (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–æ–≤ —Å–ª–æ–µ–≤
        total_flops = 0
        
        for module in model.modules():
            if isinstance(module, nn.Linear):
                total_flops += module.in_features * module.out_features
            elif isinstance(module, nn.Conv2d):
                kernel_flops = module.kernel_size[0] * module.kernel_size[1] * module.in_channels
                output_elements = (input_shape[1] // module.stride[0]) * (input_shape[2] // module.stride[1])
                total_flops += kernel_flops * output_elements * module.out_channels
        
        return total_flops

class ModelQuantizer:
    """–ö–≤–∞–Ω—Ç–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def quantize_model(self, model: nn.Module, quantization_type: str = "dynamic") -> nn.Module:
        """–ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        try:
            if quantization_type == "dynamic":
                return self._dynamic_quantization(model)
            elif quantization_type == "static":
                return self._static_quantization(model)
            elif quantization_type == "qat":  # Quantization Aware Training
                return self._qat_quantization(model)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏: {quantization_type}")
        
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏: {e}")
            return model
    
    def _dynamic_quantization(self, model: nn.Module) -> nn.Module:
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è"""
        quantized_model = torch.quantization.quantize_dynamic(
            model, {nn.Linear, nn.LSTM, nn.GRU}, dtype=torch.qint8
        )
        return quantized_model
    
    def _static_quantization(self, model: nn.Module) -> nn.Module:
        """–°—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è"""
        model.eval()
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        torch.quantization.prepare(model, inplace=True)
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –Ω–∞ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        with torch.no_grad():
            for _ in range(10):
                dummy_input = torch.randn(1, 128)  # –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                model(dummy_input)
        
        quantized_model = torch.quantization.convert(model, inplace=False)
        return quantized_model
    
    def _qat_quantization(self, model: nn.Module) -> nn.Module:
        """Quantization Aware Training"""
        model.train()
        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        torch.quantization.prepare_qat(model, inplace=True)
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        model.eval()
        quantized_model = torch.quantization.convert(model, inplace=False)
        return quantized_model

class ModelPruner:
    """–û–±—Ä–µ–∑–∞—Ç–µ–ª—å –º–æ–¥–µ–ª–µ–π (pruning)"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def prune_model(self, model: nn.Module, pruning_ratio: float = 0.5, 
                         structured: bool = False) -> nn.Module:
        """–û–±—Ä–µ–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            if structured:
                return self._structured_pruning(model, pruning_ratio)
            else:
                return self._unstructured_pruning(model, pruning_ratio)
        
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–µ–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return model
    
    def _unstructured_pruning(self, model: nn.Module, pruning_ratio: float) -> nn.Module:
        """–ù–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞"""
        import torch.nn.utils.prune as prune
        
        parameters_to_prune = []
        for module in model.modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                parameters_to_prune.append((module, 'weight'))
        
        prune.global_unstructured(
            parameters_to_prune,
            pruning_method=prune.L1Unstructured,
            amount=pruning_ratio,
        )
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –º–∞—Å–æ–∫ pruning (–¥–µ–ª–∞–µ—Ç –æ–±—Ä–µ–∑–∫—É –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π)
        for module, param_name in parameters_to_prune:
            prune.remove(module, param_name)
        
        return model
    
    def _structured_pruning(self, model: nn.Module, pruning_ratio: float) -> nn.Module:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞"""
        import torch.nn.utils.prune as prune
        
        for module in model.modules():
            if isinstance(module, nn.Linear):
                prune.ln_structured(module, name='weight', amount=pruning_ratio, n=2, dim=0)
            elif isinstance(module, nn.Conv2d):
                prune.ln_structured(module, name='weight', amount=pruning_ratio, n=2, dim=0)
        
        return model

class HyperparameterOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def optimize_hyperparameters(self, model_factory: callable, 
                                     train_data: Any, val_data: Any,
                                     param_space: Dict[str, List[Any]],
                                     max_trials: int = 20) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        best_score = float('-inf')
        best_params = None
        trial_results = []
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_combinations = self._generate_param_combinations(param_space, max_trials)
        
        for trial_idx, params in enumerate(param_combinations):
            self.logger.info(f"üîç Trial {trial_idx + 1}/{len(param_combinations)}: {params}")
            
            try:
                # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                score = await self._evaluate_params(model_factory, params, train_data, val_data)
                
                trial_results.append({
                    'trial': trial_idx + 1,
                    'params': params,
                    'score': score
                })
                
                if score > best_score:
                    best_score = score
                    best_params = params
                    self.logger.info(f"‚ú® –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {score:.4f}")
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –≤ trial {trial_idx + 1}: {e}")
        
        return {
            'best_params': best_params,
            'best_score': best_score,
            'trial_results': trial_results
        }
    
    def _generate_param_combinations(self, param_space: Dict[str, List[Any]], 
                                   max_trials: int) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        import random
        from itertools import product
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        param_names = list(param_space.keys())
        param_values = list(param_space.values())
        
        all_combinations = list(product(*param_values))
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ trials
        if len(all_combinations) > max_trials:
            selected_combinations = random.sample(all_combinations, max_trials)
        else:
            selected_combinations = all_combinations
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–∏
        combinations = []
        for combo in selected_combinations:
            param_dict = dict(zip(param_names, combo))
            combinations.append(param_dict)
        
        return combinations
    
    async def _evaluate_params(self, model_factory: callable, params: Dict[str, Any],
                             train_data: Any, val_data: Any) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        model = model_factory(**params)
        
        # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        await asyncio.sleep(0.1)  # –ò–º–∏—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –æ—Ü–µ–Ω–∫—É (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ - —Ä–µ–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        import random
        return random.uniform(0.7, 0.95)

class MemoryOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –ø–∞–º—è—Ç–∏"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    async def optimize_memory_usage(self, model: nn.Module) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        optimizations = []
        
        # Gradient checkpointing
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
            optimizations.append("gradient_checkpointing")
        
        # Mixed precision
        optimizations.append("mixed_precision_ready")
        
        # Memory efficient attention (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
        optimizations.extend(self._optimize_attention_layers(model))
        
        # Activation checkpointing
        optimizations.extend(self._setup_activation_checkpointing(model))
        
        return {
            'optimizations_applied': optimizations,
            'estimated_memory_reduction': len(optimizations) * 0.15  # 15% –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        }
    
    def _optimize_attention_layers(self, model: nn.Module) -> List[str]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è"""
        optimizations = []
        
        for name, module in model.named_modules():
            if 'attention' in name.lower() or isinstance(module, nn.MultiheadAttention):
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ flash attention –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
                optimizations.append(f"attention_optimization_{name}")
        
        return optimizations
    
    def _setup_activation_checkpointing(self, model: nn.Module) -> List[str]:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ checkpointing –∞–∫—Ç–∏–≤–∞—Ü–∏–π"""
        optimizations = []
        
        # –ü–æ–∏—Å–∫ —Å–ª–æ–µ–≤ –¥–ª—è checkpointing
        for name, module in model.named_modules():
            if isinstance(module, (nn.TransformerEncoderLayer, nn.TransformerDecoderLayer)):
                optimizations.append(f"checkpoint_{name}")
        
        return optimizations

class AutoMLOptimizer:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self):
        self.profiler = ModelProfiler()
        self.quantizer = ModelQuantizer()
        self.pruner = ModelPruner()
        self.hyperopt = HyperparameterOptimizer()
        self.memory_opt = MemoryOptimizer()
        self.logger = logging.getLogger(__name__)
    
    async def auto_optimize_model(self, model: nn.Module, input_shape: Tuple[int, ...],
                                device: torch.device, optimization_goals: List[str] = None) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        if optimization_goals is None:
            optimization_goals = ["speed", "memory", "size"]
        
        results = {}
        optimized_model = model
        
        # –ù–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        initial_profile = await self.profiler.profile_model(model, input_shape, device)
        results['initial_profile'] = initial_profile
        
        self.logger.info(f"üìä –ù–∞—á–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
        self.logger.info(f"   –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {initial_profile['inference_time_ms']:.2f}ms")
        self.logger.info(f"   –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {initial_profile['model_size_mb']:.2f}MB")
        self.logger.info(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {initial_profile['memory_usage_mb']:.2f}MB")
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
        if "size" in optimization_goals:
            # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è
            self.logger.info("üî¢ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏...")
            quantized_model = await self.quantizer.quantize_model(optimized_model)
            if quantized_model != optimized_model:
                optimized_model = quantized_model
                results['quantization_applied'] = True
            
            # Pruning
            self.logger.info("‚úÇÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ pruning...")
            pruned_model = await self.pruner.prune_model(optimized_model, pruning_ratio=0.3)
            optimized_model = pruned_model
            results['pruning_applied'] = True
        
        if "memory" in optimization_goals:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
            self.logger.info("üíæ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...")
            memory_opts = await self.memory_opt.optimize_memory_usage(optimized_model)
            results['memory_optimizations'] = memory_opts
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        final_profile = await self.profiler.profile_model(optimized_model, input_shape, device)
        results['final_profile'] = final_profile
        
        # –†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π
        improvements = self._calculate_improvements(initial_profile, final_profile)
        results['improvements'] = improvements
        
        self.logger.info(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
        self.logger.info(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {improvements['speed_improvement']:.2f}x")
        self.logger.info(f"   –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: {improvements['size_reduction']:.1f}%")
        self.logger.info(f"   –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏: {improvements['memory_reduction']:.1f}%")
        
        return {
            'optimized_model': optimized_model,
            'results': results
        }
    
    def _calculate_improvements(self, initial: Dict[str, Any], 
                              final: Dict[str, Any]) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π"""
        speed_improvement = initial['inference_time_ms'] / final['inference_time_ms']
        size_reduction = (1 - final['model_size_mb'] / initial['model_size_mb']) * 100
        memory_reduction = (1 - final['memory_usage_mb'] / initial['memory_usage_mb']) * 100
        
        return {
            'speed_improvement': speed_improvement,
            'size_reduction': max(0, size_reduction),
            'memory_reduction': max(0, memory_reduction)
        }
    
    async def benchmark_optimizations(self, model: nn.Module, input_shape: Tuple[int, ...],
                                    device: torch.device) -> Dict[str, OptimizationResult]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π"""
        results = {}
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        base_profile = await self.profiler.profile_model(model, input_shape, device)
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        for quant_type in ["dynamic", "static"]:
            try:
                start_time = time.time()
                quantized = await self.quantizer.quantize_model(model.clone(), quant_type)
                quant_profile = await self.profiler.profile_model(quantized, input_shape, device)
                optimization_time = time.time() - start_time
                
                improvement = base_profile['model_size_mb'] / quant_profile['model_size_mb']
                
                results[f"quantization_{quant_type}"] = OptimizationResult(
                    optimization_type=f"quantization_{quant_type}",
                    original_metrics=base_profile,
                    optimized_metrics=quant_profile,
                    improvement_ratio=improvement,
                    optimization_time=optimization_time,
                    success=True,
                    details={"quantization_type": quant_type}
                )
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ {quant_type}: {e}")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ pruning
        for prune_ratio in [0.25, 0.5, 0.75]:
            try:
                start_time = time.time()
                pruned = await self.pruner.prune_model(model.clone(), prune_ratio)
                prune_profile = await self.profiler.profile_model(pruned, input_shape, device)
                optimization_time = time.time() - start_time
                
                improvement = base_profile['model_size_mb'] / prune_profile['model_size_mb']
                
                results[f"pruning_{prune_ratio}"] = OptimizationResult(
                    optimization_type=f"pruning_{prune_ratio}",
                    original_metrics=base_profile,
                    optimized_metrics=prune_profile,
                    improvement_ratio=improvement,
                    optimization_time=optimization_time,
                    success=True,
                    details={"pruning_ratio": prune_ratio}
                )
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ pruning {prune_ratio}: {e}")
        
        return results 